import numpy as np
import ot
import math


def Optimal_Transport(zeta, mu, num):
    """
    Calculate the optimal transportation matrix pushing zeta to mu.
    Args:
        zeta: a matrix, every row vector is a sample from the given distribution.
        mu: a matrix, every row vector is a feature vector derived from the training set.
        num: an integer, the number of input vectors.
    Returns: 
        P: a matrix, the optimal transportation matrix pushing zeta to mu, without regularization.
    """

    # Check here if there are two feature vectors that are identical???

    c = np.ones((num,)) / num
    r = np.ones((num,)) / num  # As there were no identical vectors, the discrete empirical distribution should be evenly distributed.

    M = ot.dist(mu, zeta)  # Cost matrix uses L2 norm as its cost.
    M = M / M.max()
    # print("r:",r)
    # print("c:",c)
    # print("M:",M)
    P = ot.emd(r, c, M)  # Use the non-regularized sinkhorn to compute the optimal transportation matrix.

    return P


def Swap(arr, i, j):
    """
    Swap the i-th and j-th element of arr.
    Args:
        arr: a vector.
        i: a integer, we need to swap the i-th and j-th element of arr.
        j: a integer, we need to swap the i-th and j-th element of arr.
    Returns:
        arr: a vector.
    """

    # Check here if i and j are two valid inputs???

    tmp = arr[i]
    arr[i] = arr[j]
    arr[j] = tmp

    return arr


def Sort_Heap(heap, i):
    """
    Sort the heap from the i-th element in heap.
    This is a classical heap sort algorithm.
    Args:
        heap: a vector, we need to sort it to become a max heap.
        i: an integer, the starting node of heap sort.
    Returns: 
        heap: a vector, it's a sorted max heap.
    """

    # Check here if i is valid???

    size = len(heap)
    leftChild = 2 * i + 1
    rightChild = 2 * i + 2
    max = i  # The i-th node(parent node) should be greater than its left and right children.

    if leftChild < size and heap[leftChild] > heap[max]:
        max = leftChild

    if rightChild < size and heap[rightChild] > heap[max]:  # We want to sort the heap from right to left.
        max = rightChild

    if max != i:
        heap = Swap(heap, i, max)  # One of the child nodes is greater than the parent node, we need to swap them.
        heap = Sort_Heap(heap, max)  # The heap has changed, call the sort function recursively.

    return heap


def Find_K_Min_Nums(read, k):
    """
    Find the k smallest elements in arr.
    This is an algorithm has a complexity of O(Nlogk), which is faster than computing all the distances and sorting them.
    Args:
        read: a vector, the input vector from whom we find the k smallest elements.
        k: an integer, the number of disired smallest elements.
    Returns: 
        heap: a vector, it's a max heap containing k elements.
    """

    # Check here if k is a valid input (integer, k > 1 and k <= len(arr))???

    heap = read[0:k]  # Initialize: read the first k elements of the input vector and put them into the current heap.

    # Note that the current heap is not sorted, we need to sort it.
    for i in range(int(k / 2) - 1, -1, -1):  # int(k/2) - 1 is the index of the first non-leaf node.
        heap = Sort_Heap(heap, i)

    # Read each element from the input vector. If it is less than the root, replace the root with it, then re-sort the heap.
    for i in range(k, len(read)):
        if read[i] < heap[0]:
            heap[0] = read[i]
            Sort_Heap(heap, 0)

    return heap


def Generate(zeta, mu, m, k=3):
    """
    Generate m feature vectors that seems to be drawn out of distribution mu.
    Args:
        zeta: a matrix, every row vector is a sample from the given distribution.
        mu: a matrix, every row vector is a feature vector derived from the training set.
        m: an integer, the number of generated pictures.
        k: an integer, the number of nearest neighbors when generating new feature vectors.
    Returns: 
        result: a matrix, each row vector is a pseudo-feature vector generated by the algorithm, which should be distributed around the feature manifold.
    """

    dim = zeta.shape[1]
    P = Optimal_Transport(zeta, mu, zeta.shape[0])  # Optimal transportation matrix pushing zeta to mu, without regularization. It's a dim*dim matrix.

    new_input = np.random.randn(m, dim)
    dist = ot.dist(10 * new_input, 10 * zeta)  # The i-th row is the L2 distance between the i-th input vector and all vectors from zeta.

    # location = np.zeros(shape = (m, k)) # The (i,j) element is the index of the j-th nearest feature vector from the i-th input vecor.
    desired_vecs = np.zeros(shape=(k, dim))  # It's a temporary matrix at the i-th iteration, the j-th row is the j-th nearest feature vector from the i-th input vecor.
    result = np.zeros(shape=(m, dim))  # The i-th row is the pseudo-feature vector generated by the i-th input vector.

    for i in range(m):
        k_min = Find_K_Min_Nums(dist[i], k)  # Find the k nearest neighbors of the i-th input vector.

        for j in range(k):
            # location[i][j] = np.array(np.array(np.where(dist[i] == k_min[j])).min())) # Find the index of each neighbor.

            # Find the feature vectors at the derived locations
            desired_vecs[j] = mu[np.nonzero(P[round(len(np.array(np.where(dist[i] == k_min[j]))) * np.random.uniform(0, 1))])[0][0]]

        # Generate new feature vectors by randomly generate a linear combination of the original feature vectors at the derived locations.
        coefficient = np.random.dirichlet(np.ones(k), size=1)[0]
        result[i] = coefficient.dot(desired_vecs)

    return result

# zeta = np.array([[1,0,0],[0,1,0], [0,0,1]])
# mu = np.array([[2,3,4], [9,0,4], [8,9,1]])
# a=Generate(zeta, mu, m = 20, k = 2)
